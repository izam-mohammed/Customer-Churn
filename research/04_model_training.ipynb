{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/izam/coding/Customer-Churn'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    model_name: str\n",
    "    params: dict\n",
    "    target_col: str\n",
    "    permanent_path: str\n",
    "    auto_select: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomerChurn.constants import *\n",
    "from CustomerChurn.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    A class to manage configuration settings for this data science project.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: str = CONFIG_FILE_PATH,\n",
    "        params_filepath: str = PARAMS_FILE_PATH,\n",
    "        schema_filepath: str = SCHEMA_FILE_PATH\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager with default or provided file paths.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (str, optional): Path to the main configuration file.\n",
    "        - params_filepath (str, optional): Path to the parameters file.\n",
    "        - schema_filepath (str, optional): Path to the schema file.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        \"\"\"\n",
    "        Retrieves the configuration for the model trainer.\n",
    "\n",
    "        Returns:\n",
    "        - ModelTrainerConfig: Configuration settings for the model trainer.\n",
    "        \"\"\"\n",
    "        config = self.config.model_trainer\n",
    "        target_col = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            model_name = config.model_name,\n",
    "            params = self.params,\n",
    "            target_col=target_col.name,\n",
    "            permanent_path=config.permanent_path,\n",
    "            auto_select=config.auto_select,\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from box import ConfigBox\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from CustomerChurn import logger\n",
    "from CustomerChurn.utils.common import save_bin, save_bin_dup\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import reciprocal, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    A class for training and selecting the best machine learning model based on the provided configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        \"\"\"\n",
    "        Initializes the ModelTrainer instance with the provided configuration and loads train and test data.\n",
    "\n",
    "        Args:\n",
    "        - config (ModelTrainerConfig): Configuration settings for model training.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        train = pd.read_csv(self.config.train_data_path)\n",
    "        test = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        self.X_train= train.drop([self.config.target_col], axis=1)\n",
    "        self.y_train = train[self.config.target_col]\n",
    "        self.X_test = test.drop([self.config.target_col], axis=1)\n",
    "        self.y_test = test[self.config.target_col]   \n",
    "\n",
    "\n",
    "    def _randomized_search(self, name,clf,params, runs=50): \n",
    "        \"\"\"\n",
    "        Performs randomized search for hyperparameter tuning and evaluates the model.\n",
    "\n",
    "        Args:\n",
    "        - name (str): Name of the model for logging purposes.\n",
    "        - clf: Machine learning model.\n",
    "        - params: Hyperparameter grid or distribution for randomized search.\n",
    "        - runs (int): Number of iterations for randomized search.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[Any, float]: Tuple containing the best model and its accuracy score.\n",
    "        \"\"\"\n",
    "        rand_clf = RandomizedSearchCV(clf, params, n_iter=runs, cv=5, n_jobs=4, random_state=2, verbose=1)     \n",
    "\n",
    "        rand_clf.fit(self.X_train, self.y_train) \n",
    "        best_model = rand_clf.best_estimator_\n",
    "        \n",
    "        best_score = rand_clf.best_score_\n",
    "        logger.info(\"Trained with {} with score: {:.3f}\".format(name, best_score))\n",
    "\n",
    "        y_pred = best_model.predict(self.X_test)\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        logger.info('Predicted with {} ; Test score : {:.3f}'.format(name, accuracy))\n",
    "        \n",
    "        return best_model, accuracy\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains multiple machine learning models, selects the best one, and saves the model.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        model_params = self.config.params\n",
    "\n",
    "        models = ConfigBox({\n",
    "            \"Decision_Tree\": {\n",
    "                \"model\" : DecisionTreeClassifier(),\n",
    "                \"params\" : model_params.Decision_Tree,\n",
    "                \"auto\":{\n",
    "                        \"criterion\" : ['gini', 'entropy'],\n",
    "                        \"splitter\" : ['best', 'random'],\n",
    "                        \"max_depth\" : range( 1, 32),\n",
    "                        \"min_samples_split\" : uniform( 0.1, 1.0),\n",
    "                        \"min_samples_leaf\" : uniform( 0.1, 0.5),\n",
    "                        \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "                }\n",
    "            },\n",
    "            \"Random_Forest\": {\n",
    "                \"model\" : RandomForestClassifier(),\n",
    "                \"params\" : model_params.Random_Forest,\n",
    "                \"auto\":{\n",
    "                    'n_estimators': range(10, 200),\n",
    "                    'criterion': ['gini', 'entropy'],\n",
    "                    'max_depth': range(1, 20),\n",
    "                    'min_samples_split': range(2, 20),\n",
    "                    'min_samples_leaf': range(1, 20),\n",
    "                    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "                    'bootstrap': [True, False]\n",
    "                }\n",
    "            },\n",
    "            \"SVC\": {\n",
    "                \"model\" : SVC(),\n",
    "                \"params\" : model_params.SVC,\n",
    "                \"auto\":{\n",
    "                    'C': reciprocal(0.1, 10),  \n",
    "                    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                    'degree': range(2, 7),\n",
    "                    'gamma': ['scale', 'auto'] + list(uniform(0.1, 1.0).rvs(10)),\n",
    "                    'coef0': uniform(-1, 1)\n",
    "                }\n",
    "            },\n",
    "            \"LogisticRegression\":{\n",
    "                \"model\" : LogisticRegression(),\n",
    "                \"params\" : model_params.LogisticRegression,\n",
    "                \"auto\" : {\n",
    "                        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                        'C': uniform(0.1, 10), \n",
    "                        'fit_intercept': [True, False],\n",
    "                        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                        'max_iter': range(50, 500, 50)\n",
    "                }\n",
    "            },\n",
    "            \"MultinomialNB\":{\n",
    "                \"model\" : MultinomialNB(),\n",
    "                \"params\" : model_params.MultinomialNB,\n",
    "                \"auto\": {\n",
    "                        'alpha': uniform(0.1, 2.0), \n",
    "                        'fit_prior': [True, False],\n",
    "                        'class_prior': [None, list(uniform(0.1, 1.0).rvs(3))] \n",
    "                    }\n",
    "            },\n",
    "            \"GradientBoost\":{\n",
    "                \"model\": GradientBoostingClassifier(),\n",
    "                \"params\" : model_params.GradientBoost,\n",
    "                \"auto\": {\n",
    "                        'learning_rate': uniform(0.01, 0.2),  # Uniform distribution for learning_rate\n",
    "                        'n_estimators': range(50, 200, 30),\n",
    "                        'max_depth': range(3, 10),\n",
    "                        'min_samples_split': uniform(0.1, 1.0),  # Uniform distribution for min_samples_split\n",
    "                        'min_samples_leaf': uniform(0.1, 0.5),   # Uniform distribution for min_samples_leaf\n",
    "                        'subsample': uniform(0.5, 1.0),           # Uniform distribution for subsample\n",
    "                        'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "                }\n",
    "            },\n",
    "            \"AdaBoost\":{\n",
    "                \"model\" : AdaBoostClassifier(),\n",
    "                \"params\" : model_params.AdaBoost,\n",
    "                \"auto\":{\n",
    "                        'n_estimators': range(50, 200),\n",
    "                        'learning_rate': uniform(0.01, 1.0),  # Uniform distribution for learning_rate\n",
    "                        'algorithm': ['SAMME', 'SAMME.R'],\n",
    "                        'base_estimator': [None, DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2)],\n",
    "                }\n",
    "            },\n",
    "            \"XGBoost\":{\n",
    "                \"model\" : XGBClassifier(),\n",
    "                \"params\" : model_params.XGBoost,\n",
    "                \"auto\":{\n",
    "                        'learning_rate': uniform(0.01, 0.2),\n",
    "                        'n_estimators': range(50, 200),\n",
    "                        'max_depth': range(3, 10),\n",
    "                        'min_child_weight': range(1, 10),\n",
    "                        'subsample': uniform(0.5, 1.0),\n",
    "                        'colsample_bytree': uniform(0.5, 1.0),\n",
    "                        'gamma': uniform(0, 1),\n",
    "                        'reg_alpha': uniform(0, 1),\n",
    "                        'reg_lambda': uniform(0, 1),\n",
    "                        'scale_pos_weight': range(1, 10),\n",
    "                        'base_score': uniform(0.1, 0.9),\n",
    "                        'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "                        'n_jobs': [-1],\n",
    "                        'random_state': range(1, 100),\n",
    "                }\n",
    "            },\n",
    "        })\n",
    "\n",
    "        create_directories([os.path.join(self.config.root_dir, \"models\")])\n",
    "        trained_models = []\n",
    "        for model in models:\n",
    "            clf = models[model].model\n",
    "            params = models[model].params\n",
    "\n",
    "            if self.config.auto_select:\n",
    "                params = models[model].auto\n",
    "            else:\n",
    "                params = models[model].params\n",
    "            if model==\"XGBoost\":\n",
    "                clf_model, score = self._randomized_search(name=str(model) ,clf=clf, params=params, runs=300)\n",
    "            else:\n",
    "                clf_model, score = self._randomized_search(name=str(model) ,clf=clf, params=params)\n",
    "            trained_models.append((clf_model, score))\n",
    "\n",
    "            save_bin(data=clf_model, path=Path(os.path.join(self.config.root_dir, f\"models/{str(model)}.joblib\")))\n",
    "        \n",
    "        trained_models = sorted(trained_models, key=lambda x:x[1], reverse=True)  # [(model, score), (model, score), ..]\n",
    "        best_model = trained_models[0][0]  # taking the model\n",
    "\n",
    "        save_bin(data=best_model, path=Path(os.path.join(self.config.root_dir, self.config.model_name)))\n",
    "        save_bin_dup(data=best_model, path=Path(os.path.join(self.config.permanent_path, \"model.joblib\")))\n",
    "\n",
    "        best_model_name = str(best_model)[:str(best_model).find(\"(\")]\n",
    "        best_model_score = round(trained_models[0][1], 3)\n",
    "        logger.info(f\"Saved main model as {best_model_name}, with score - {best_model_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-01 16:47:32,317: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-02-01 16:47:32,319: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-01 16:47:32,323: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-01 16:47:32,324: INFO: common: created directory at: artifacts]\n",
      "[2024-02-01 16:47:32,325: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-02-01 16:47:32,350: INFO: common: created directory at: artifacts/model_trainer/models]\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[2024-02-01 16:47:33,662: INFO: 3939585448: Trained with Decision_Tree with score: 0.751]\n",
      "[2024-02-01 16:47:33,664: INFO: 3939585448: Predicted with Decision_Tree ; Test score : 0.746]\n",
      "[2024-02-01 16:47:33,665: INFO: common: binary file saved at: artifacts/model_trainer/models/Decision_Tree.joblib]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "70 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1.047730611527878 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1.0350357305790105 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.72949956        nan 0.73003195        nan 0.73003195\n",
      "        nan 0.73003195 0.73003195 0.73003195        nan 0.73003195\n",
      " 0.73003195 0.73003195        nan        nan 0.73003195 0.73003195\n",
      " 0.73003195        nan 0.73003195 0.73003195        nan 0.75097912\n",
      " 0.73003195 0.73003195 0.73003195        nan 0.73003195 0.73003195\n",
      " 0.73003195        nan 0.73003195 0.73003195 0.73003195 0.73003195\n",
      " 0.73003195        nan 0.73003195 0.73003195        nan 0.73003195\n",
      " 0.73003195        nan 0.73003195 0.73003195 0.73003195 0.73003195\n",
      " 0.73003195 0.73003195]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "70 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "53 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "17 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/izam/anaconda3/envs/customer-churn/lib/python3.11/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.80049913        nan 0.73802169        nan 0.79286762\n",
      " 0.79464099        nan 0.75417044 0.79552861 0.79907865 0.77529444\n",
      "        nan 0.73003195        nan        nan        nan        nan\n",
      " 0.79943341 0.78949348 0.79836958 0.73802169 0.7932216  0.79623862\n",
      " 0.79588448        nan 0.79677132 0.78878441        nan 0.78239467\n",
      " 0.79819102 0.79499607 0.79890166 0.79623878 0.77387459 0.77600492\n",
      " 0.78807425 0.79872514        nan 0.79340017 0.79499654 0.79641671\n",
      " 0.79393366 0.79446431        nan 0.79322144        nan 0.79268795\n",
      " 0.78700853 0.79925611]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-01 16:47:56,485: INFO: 3939585448: Trained with Random_Forest with score: 0.800]\n",
      "[2024-02-01 16:47:56,503: INFO: 3939585448: Predicted with Random_Forest ; Test score : 0.815]\n",
      "[2024-02-01 16:47:56,530: INFO: common: binary file saved at: artifacts/model_trainer/models/Random_Forest.joblib]\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customerchurn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
